{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16dd1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import polars as pl\n",
    "from polars._typing import PolarsDataType, SchemaDict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_dtypes(dtype1: PolarsDataType, dtype2: PolarsDataType) -> PolarsDataType:\n",
    "    \"\"\"Recursively merges two Polars data types into a supertype.\"\"\"\n",
    "    # If types are the same, no merge needed\n",
    "    if dtype1 == dtype2:\n",
    "        return dtype1\n",
    "    \n",
    "    # Null type is superseded by any other type\n",
    "    if isinstance(dtype1, pl.Null):\n",
    "        return dtype2\n",
    "    if isinstance(dtype2, pl.Null):\n",
    "        return dtype1\n",
    "\n",
    "    # Recursively merge for List types\n",
    "    if isinstance(dtype1, pl.List) and isinstance(dtype2, pl.List):\n",
    "        merged_inner = _merge_dtypes(dtype1.inner, dtype2.inner)\n",
    "        return pl.List(merged_inner)\n",
    "\n",
    "    # Recursively merge for Struct types\n",
    "    if isinstance(dtype1, pl.Struct) and isinstance(dtype2, pl.Struct):\n",
    "        # Combine fields from both structs\n",
    "        merged_fields = dtype1.to_schema()\n",
    "        for field_name, field_dtype in dtype2.to_schema().items():\n",
    "            merged_fields[field_name] = _merge_dtypes(merged_fields[field_name], field_dtype) if field_name in merged_fields else field_dtype\n",
    "        return pl.Struct(merged_fields)\n",
    "\n",
    "    # For other types, use Polars' built-in supertype casting\n",
    "    try:\n",
    "        # Create dummy series and find the supertype upon concatenation\n",
    "        super_type = pl.concat(\n",
    "            [pl.Series([None], dtype=dtype1), pl.Series([None], dtype=dtype2)],\n",
    "            how=\"diagonal_relaxed\",\n",
    "        ).dtype\n",
    "        return super_type\n",
    "    except Exception:\n",
    "        # If Polars can't find a common supertype (e.g., Int64 and Struct), raise an error\n",
    "        raise TypeError(f\"Could not merge incompatible types: {dtype1} and {dtype2}\")\n",
    "\n",
    "\n",
    "def merge_schemas(schemas: list[SchemaDict]) -> SchemaDict:\n",
    "    \"\"\"\n",
    "    Merges a list of Polars schemas into a single, generalized schema.\n",
    "\n",
    "    Args:\n",
    "        schemas: A list of schema dictionaries (e.g., from `df.schema`).\n",
    "\n",
    "    Returns:\n",
    "        A single schema dictionary that covers all fields and types.\n",
    "    \"\"\"\n",
    "    if not schemas:\n",
    "        return {}\n",
    "\n",
    "    # Start with the first schema as the base\n",
    "    merged_schema = dict(schemas[0])\n",
    "\n",
    "    # Iteratively merge the remaining schemas\n",
    "    for schema in schemas[1:]:\n",
    "        for field_name, field_dtype in schema.items():\n",
    "            if field_name in merged_schema:\n",
    "                # Field exists, merge the data types\n",
    "                existing_dtype = merged_schema[field_name]\n",
    "                merged_schema[field_name] = _merge_dtypes(existing_dtype, field_dtype)\n",
    "            else:\n",
    "                # New field, just add it to the schema\n",
    "                merged_schema[field_name] = field_dtype\n",
    "    \n",
    "    return merged_schema\n",
    "\n",
    "\n",
    "def extract_articulations_optimized(fp: str | Path, schema: pl.Schema) -> pl.DataFrame:\n",
    "    path = Path(fp)\n",
    "    # Robust path parsing\n",
    "    # print(path.parts)\n",
    "    uni = int(path.parts[-2])\n",
    "    cc = int(path.parts[-1].split(\"to\")[0])\n",
    "\n",
    "    if \"prefixes\" in fp:\n",
    "        df = pl.read_json(fp, schema=schema).explode(\"articulations\")\n",
    "    else:\n",
    "        df = pl.read_json(fp, schema=schema).rename({\"articulation\": \"articulations\"})\n",
    "\n",
    "    return (\n",
    "        df.select(\n",
    "            # Use select for efficient, simultaneous column creation\n",
    "            pl.col(\"articulations\").struct.field(\"course\").struct.field(\"courseIdentifierParentId\").alias(\"course_id\"),\n",
    "            pl.col(\"articulations\").struct.field(\"series\"),\n",
    "            pl.col(\"articulations\").struct.field(\"sendingArticulation\")\n",
    "        )\n",
    "        .with_columns(pl.col(\"series\").struct.field(\"courses\"))\n",
    "        .explode(\"courses\")\n",
    "        .with_columns(\n",
    "            # This logic remains largely the same, but benefits from a cleaner starting point\n",
    "            course_id=pl.coalesce(\n",
    "                \"course_id\",\n",
    "                pl.col(\"courses\").struct.field(\"courseIdentifierParentId\")\n",
    "            ),\n",
    "            # The 'when' is necessary to convert empty lists to null for later dropping\n",
    "            items=pl.when(pl.col(\"sendingArticulation\").struct.field(\"items\").list.len() > 0)\n",
    "                 .then(pl.col(\"sendingArticulation\").struct.field(\"items\"))\n",
    "                 .otherwise(None),\n",
    "            cc=pl.lit(cc),\n",
    "            uni=pl.lit(uni)\n",
    "        )\n",
    "        .drop_nulls(subset=\"items\") # Single drop_nulls is sufficient for this column\n",
    "        .with_columns(\n",
    "            articulation=pl.col(\"items\").list.eval(\n",
    "                pl.struct(\n",
    "                    conj=pl.element().struct.field(\"courseConjunction\"),\n",
    "                    items=pl.element().struct.field(\"items\").list.eval(\n",
    "                        pl.element().struct.field(\"courseIdentifierParentId\")\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            # Simplified and more idiomatic conditional logic\n",
    "            groupConj=(\n",
    "                pl.col(\"sendingArticulation\")\n",
    "                .struct.field(\"courseGroupConjunctions\")\n",
    "                .list.first()\n",
    "                .struct.field(\"groupConjunction\")\n",
    "                .fill_null(\"Or\") # Much cleaner than when/then/otherwise\n",
    "            )\n",
    "        )\n",
    "        .drop(\"series\", \"courses\", \"sendingArticulation\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb860cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_list_prefix = [pl.read_json(source=fp, infer_schema_length=None).schema for fp in glob(\"data/*/*prefixes.json\")]\n",
    "schema_list_major  = [pl.read_json(source=fp, infer_schema_length=None).schema for fp in glob(\"data/*/*majors.json\")]\n",
    "\n",
    "schema_prefix = merge_schemas(schema_list_prefix)\n",
    "schema_major  = merge_schemas(schema_list_major)\n",
    "\n",
    "del schema_list_prefix, schema_list_major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c37a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes_agg = pl.concat([\n",
    "    extract_articulations_optimized(fp, schema_prefix) \n",
    "    for fp in glob(\"data/*/*-prefixes.json\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3b33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "majors_agg = pl.concat([\n",
    "    extract_articulations_optimized(fp, schema_major) \n",
    "    for fp in glob(\"data/*/*-majors.json\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2142d656",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prefixes_agg.sort(by=[\"cc\", \"uni\", \"course_id\"]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "majors_agg.sort(by=[\"cc\", \"uni\", \"course_id\"]).unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
