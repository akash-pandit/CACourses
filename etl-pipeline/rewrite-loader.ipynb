{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "1cde1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import itertools\n",
    "from functools import lru_cache\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "1156c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTDIR = Path(\"/home/akash/Main/projects/CACourses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def extract_articulations(fp: Path, schema: pl.Schema | None) -> pl.DataFrame:\n",
    "    uni = int(fp.parts[-2])\n",
    "    cc  = int(fp.parts[-1].split('to')[0])\n",
    "\n",
    "    lf = pl.read_json(source=fp, schema=schema).lazy()\n",
    "\n",
    "    # Normalize structure (Explode list vs Rename single)\n",
    "    if \"prefixes\" in str(fp):\n",
    "        lf = lf.explode(\"articulations\")\n",
    "    else:\n",
    "        lf = lf.rename({\"articulation\": \"articulations\"})\n",
    "\n",
    "    return (\n",
    "        lf\n",
    "        # 1. Filter empty articulations immediately\n",
    "        .filter(\n",
    "            pl.col(\"articulations\")\n",
    "            .struct.field(\"sendingArticulation\")\n",
    "            .struct.field(\"items\")\n",
    "            .list.len() > 0\n",
    "        )\n",
    "        # 2. Extract Fields & Merge Source IDs\n",
    "        .select(\n",
    "            # Extract Series IDs (List[Int]) and Single IDs (Int)\n",
    "            series_ids=pl.col(\"articulations\").struct.field(\"series\").struct.field(\"courses\")\n",
    "                     .list.eval(pl.element().struct.field(\"courseIdentifierParentId\")),\n",
    "            \n",
    "            root_id=pl.col(\"articulations\").struct.field(\"course\").struct.field(\"courseIdentifierParentId\"),\n",
    "\n",
    "            # Extract Destination Data\n",
    "            sending_items=pl.col(\"articulations\").struct.field(\"sendingArticulation\").struct.field(\"items\"),\n",
    "            \n",
    "            # Global Conjunction\n",
    "            global_conj=(\n",
    "                pl.col(\"articulations\")\n",
    "                .struct.field(\"sendingArticulation\")\n",
    "                .struct.field(\"courseGroupConjunctions\")\n",
    "                .list.first()\n",
    "                .struct.field(\"groupConjunction\")\n",
    "                .fill_null(\"Or\")\n",
    "            )\n",
    "        )\n",
    "        # 3. Safe Explode Logic\n",
    "        # Coalesce series list with root_id (wrapped in a list) so we never drop rows\n",
    "        .with_columns(\n",
    "            source_id_list=pl.coalesce(\n",
    "                pl.col(\"series_ids\"), \n",
    "                pl.concat_list(pl.col(\"root_id\")) \n",
    "            )\n",
    "        )\n",
    "        .explode(\"source_id_list\") # Now safe to explode\n",
    "        \n",
    "        # 4. Final Construction\n",
    "        .select(\n",
    "            cc=pl.lit(cc),\n",
    "            uni=pl.lit(uni),\n",
    "            course_id=pl.col(\"source_id_list\"),\n",
    "            articulation=pl.struct(\n",
    "                conj=pl.col(\"global_conj\"),\n",
    "                items=pl.col(\"sending_items\").list.eval(\n",
    "                    pl.struct(\n",
    "                        conj=pl.element().struct.field(\"courseConjunction\"),\n",
    "                        items=pl.element().struct.field(\"items\").list.eval(\n",
    "                            pl.element().struct.field(\"courseIdentifierParentId\")\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .group_by(\n",
    "            pl.col(\"course_id\"),\n",
    "            pl.col(\"cc\"),\n",
    "            pl.col(\"uni\")\n",
    "        ).all()\n",
    "        .select(\n",
    "            course_id=pl.col(\"course_id\"),\n",
    "            cc=pl.col(\"cc\"),\n",
    "            uni=pl.col(\"uni\"),\n",
    "            articulation=pl.struct(\n",
    "                conj=pl.lit(\"Or\"),\n",
    "                items=pl.col(\"articulation\")\n",
    "            )\n",
    "        )\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "\n",
    "def to_dnf(node):\n",
    "    \"\"\"\n",
    "    Recursively flattens a logic tree of arbitrary depth into a 2D matrix.\n",
    "    Returns: List[List[int]] (Disjunctive Normal Form)\n",
    "    \"\"\"\n",
    "    # base: no conjunctions\n",
    "    if not isinstance(node, dict):\n",
    "        return [[node]] if node is not None else []\n",
    "\n",
    "    # extract logic & children\n",
    "    conj = node.get(\"conj\")\n",
    "    children = node.get(\"items\")\n",
    "    if not children:\n",
    "        return []\n",
    "\n",
    "    # base: And/Or depth=1\n",
    "    if all(isinstance(child, int) for child in children):\n",
    "        if conj == \"And\":\n",
    "            return [children]  # And(1, 2) -> [[1, 2]]\n",
    "        else: \n",
    "            return [[x] for x in children]  # Or(1, 2) -> [[1], [2]]\n",
    "\n",
    "    # recurse children to child matrices\n",
    "    child_matrices = [to_dnf(child) for child in children]\n",
    "\n",
    "    # DNF algorithm: apply associative property on Or(1, 2, Or(3))\n",
    "    if conj == \"Or\":\n",
    "        merged_matrix = []\n",
    "        for matrix in child_matrices:\n",
    "            merged_matrix.extend(matrix)\n",
    "        return merged_matrix\n",
    "\n",
    "    # DNF algorithm: apply distributive property (And over Or)\n",
    "    #    (A OR B) AND (C OR D)\n",
    "    # => (A AND (C OR D)) OR (B AND (C OR D))\n",
    "    # => (A AND C) OR (A AND D) OR (B AND C) OR (B AND D)\n",
    "    elif conj == \"And\":\n",
    "        product = itertools.product(*child_matrices)\n",
    "        \n",
    "        merged_matrix = []\n",
    "        for combination in product:\n",
    "            new_clause = []\n",
    "            for clause in combination:\n",
    "                new_clause.extend(clause)\n",
    "            merged_matrix.append(new_clause)\n",
    "            \n",
    "        return merged_matrix\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def _resolve_supertype(dtype1: pl.DataType, dtype2: pl.DataType) -> pl.DataType:\n",
    "    \"\"\"\n",
    "    Caches the expensive supertype resolution.\n",
    "    This bypasses creating dummy Series for repetitive primitive merges\n",
    "    (e.g., merging Int64 and Float64 thousands of times).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # diagonal_relaxed allows Polars to determine the common supertype\n",
    "        return pl.concat(\n",
    "            [pl.Series([None], dtype=dtype1), pl.Series([None], dtype=dtype2)],\n",
    "            how=\"diagonal_relaxed\",\n",
    "        ).dtype\n",
    "    except Exception:\n",
    "        raise TypeError(f\"Could not merge incompatible types: {dtype1} and {dtype2}\")\n",
    "    return\n",
    "\n",
    "\n",
    "def _merge_dtypes_optimized(dtype1: pl.DataType, dtype2: pl.DataType) -> pl.DataType:\n",
    "    \"\"\"Optimized recursive merge.\"\"\"\n",
    "    # 1. Identity Check (Fastest exit)\n",
    "    if dtype1 == dtype2:\n",
    "        return dtype1\n",
    "\n",
    "    # 2. Null Handling\n",
    "    if isinstance(dtype1, pl.Null): return dtype2\n",
    "    if isinstance(dtype2, pl.Null): return dtype1\n",
    "\n",
    "    # 3. Recursive List Merge\n",
    "    if isinstance(dtype1, pl.List) and isinstance(dtype2, pl.List):\n",
    "        return pl.List(_merge_dtypes_optimized(dtype1.inner, dtype2.inner))\n",
    "\n",
    "    # 4. Recursive Struct Merge\n",
    "    if isinstance(dtype1, pl.Struct) and isinstance(dtype2, pl.Struct):\n",
    "        # Convert both to dictionaries once\n",
    "        f1 = dtype1.to_schema()\n",
    "        f2 = dtype2.to_schema()\n",
    "        \n",
    "        # Start with f1's fields\n",
    "        merged_fields = f1.copy()\n",
    "        \n",
    "        # Only iterate over fields in f2\n",
    "        for key, type2 in f2.items():\n",
    "            type1 = merged_fields.get(key)\n",
    "            if type1 is not None:\n",
    "                # Recursively merge only if types differ\n",
    "                if type1 != type2:\n",
    "                    merged_fields[key] = _merge_dtypes_optimized(type1, type2)\n",
    "            else:\n",
    "                # New field from f2\n",
    "                merged_fields[key] = type2\n",
    "        \n",
    "        return pl.Struct(merged_fields)\n",
    "\n",
    "    # 5. Cached Primitive Resolution\n",
    "    # We use the cached function for scalar types (Int, Float, String, etc.)\n",
    "    return _resolve_supertype(dtype1, dtype2)\n",
    "\n",
    "\n",
    "def merge_schemas(schemas: list[pl.Schema]) -> pl.Schema:\n",
    "    \"\"\"\n",
    "    Optimized schema merging.\n",
    "    \"\"\"\n",
    "    if not schemas:\n",
    "        return pl.Schema()\n",
    "\n",
    "    # Convert the first schema to a mutable dictionary immediately\n",
    "    # casting to dict() is cheaper than repetitive lookups on a Schema object\n",
    "    current_schema_map = dict(schemas[0])\n",
    "\n",
    "    for schema in schemas[1:]:\n",
    "        # Iterate only over the new schema's items\n",
    "        for field_name, new_dtype in schema.items():\n",
    "            existing_dtype = current_schema_map.get(field_name)\n",
    "            \n",
    "            if existing_dtype is None:\n",
    "                # Fast path: New field\n",
    "                current_schema_map[field_name] = new_dtype\n",
    "            elif existing_dtype != new_dtype:\n",
    "                # Slow path: Conflict resolution\n",
    "                current_schema_map[field_name] = _merge_dtypes_optimized(existing_dtype, new_dtype)\n",
    "    \n",
    "    return pl.Schema(current_schema_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b875f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_list_prefix = [\n",
    "    pl.read_json(\n",
    "        source=fp,\n",
    "        infer_schema_length=None\n",
    "    ).schema \n",
    "    for fp \n",
    "    in PROJECTDIR.glob(\"data/*/*prefixes.json\")\n",
    "]\n",
    "schema_prefix = merge_schemas(schema_list_prefix)\n",
    "del schema_list_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "48ada01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (128, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>course_id</th><th>cc</th><th>uni</th><th>articulation</th></tr><tr><td>i64</td><td>i32</td><td>i32</td><td>object</td></tr></thead><tbody><tr><td>25929</td><td>101</td><td>129</td><td>[[338761]]</td></tr><tr><td>25981</td><td>101</td><td>129</td><td>[[41543], [336082, 41543]]</td></tr><tr><td>27466</td><td>101</td><td>129</td><td>[[309095, 287142], [309095, 304417, 287142], [309095]]</td></tr><tr><td>27468</td><td>101</td><td>129</td><td>[[251680, 287129], [251680]]</td></tr><tr><td>27486</td><td>101</td><td>129</td><td>[[304417], [287142, 304417], [309095, 304417, 287142]]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>359040</td><td>101</td><td>129</td><td>[[307776], [280607, 307776], [307918, 359112, 307776]]</td></tr><tr><td>359198</td><td>101</td><td>129</td><td>[[359100], [359100, 358890]]</td></tr><tr><td>359201</td><td>101</td><td>129</td><td>[[281073]]</td></tr><tr><td>359225</td><td>101</td><td>129</td><td>[[358952], [280826, 358952]]</td></tr><tr><td>387349</td><td>101</td><td>129</td><td>[[355788], [355777]]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (128, 4)\n",
       "┌───────────┬─────┬─────┬─────────────────────────────────┐\n",
       "│ course_id ┆ cc  ┆ uni ┆ articulation                    │\n",
       "│ ---       ┆ --- ┆ --- ┆ ---                             │\n",
       "│ i64       ┆ i32 ┆ i32 ┆ object                          │\n",
       "╞═══════════╪═════╪═════╪═════════════════════════════════╡\n",
       "│ 25929     ┆ 101 ┆ 129 ┆ [[338761]]                      │\n",
       "│ 25981     ┆ 101 ┆ 129 ┆ [[41543], [336082, 41543]]      │\n",
       "│ 27466     ┆ 101 ┆ 129 ┆ [[309095, 287142], [309095, 30… │\n",
       "│ 27468     ┆ 101 ┆ 129 ┆ [[251680, 287129], [251680]]    │\n",
       "│ 27486     ┆ 101 ┆ 129 ┆ [[304417], [287142, 304417], [… │\n",
       "│ …         ┆ …   ┆ …   ┆ …                               │\n",
       "│ 359040    ┆ 101 ┆ 129 ┆ [[307776], [280607, 307776], [… │\n",
       "│ 359198    ┆ 101 ┆ 129 ┆ [[359100], [359100, 358890]]    │\n",
       "│ 359201    ┆ 101 ┆ 129 ┆ [[281073]]                      │\n",
       "│ 359225    ┆ 101 ┆ 129 ┆ [[358952], [280826, 358952]]    │\n",
       "│ 387349    ┆ 101 ┆ 129 ┆ [[355788], [355777]]            │\n",
       "└───────────┴─────┴─────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpath = Path(\"/home/akash/Main/projects/CACourses/data/129/101to129-prefixes.json\")\n",
    "\n",
    "test = extract_articulations(fp=testpath, schema=schema_prefix)\n",
    "\n",
    "dnf_matrix_df = test.with_columns(\n",
    "    pl.col(\"articulation\")\n",
    "    .map_elements(to_dnf, return_dtype=pl.Object)\n",
    ")\n",
    "\n",
    "dnf_matrix_df.sort(\"course_id\", \"cc\", \"uni\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3603a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "7f97d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (382_422, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>course_id</th><th>cc</th><th>uni</th><th>articulation</th></tr><tr><td>i64</td><td>i32</td><td>i32</td><td>list[list[i64]]</td></tr></thead><tbody><tr><td>259939</td><td>114</td><td>7</td><td>[[201884, 207755]]</td></tr><tr><td>353759</td><td>133</td><td>46</td><td>[[205081, 212781], [204750, 212781]]</td></tr><tr><td>377659</td><td>153</td><td>1</td><td>[[372203], [372214]]</td></tr><tr><td>174287</td><td>94</td><td>88</td><td>[[302608]]</td></tr><tr><td>99057</td><td>109</td><td>24</td><td>[[28143]]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>73014</td><td>56</td><td>29</td><td>[[76319]]</td></tr><tr><td>85995</td><td>150</td><td>88</td><td>[[350038]]</td></tr><tr><td>353765</td><td>200</td><td>46</td><td>[[377121]]</td></tr><tr><td>288885</td><td>107</td><td>7</td><td>[[273944]]</td></tr><tr><td>359401</td><td>146</td><td>132</td><td>[[346288]]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (382_422, 4)\n",
       "┌───────────┬─────┬─────┬─────────────────────────────────┐\n",
       "│ course_id ┆ cc  ┆ uni ┆ articulation                    │\n",
       "│ ---       ┆ --- ┆ --- ┆ ---                             │\n",
       "│ i64       ┆ i32 ┆ i32 ┆ list[list[i64]]                 │\n",
       "╞═══════════╪═════╪═════╪═════════════════════════════════╡\n",
       "│ 259939    ┆ 114 ┆ 7   ┆ [[201884, 207755]]              │\n",
       "│ 353759    ┆ 133 ┆ 46  ┆ [[205081, 212781], [204750, 21… │\n",
       "│ 377659    ┆ 153 ┆ 1   ┆ [[372203], [372214]]            │\n",
       "│ 174287    ┆ 94  ┆ 88  ┆ [[302608]]                      │\n",
       "│ 99057     ┆ 109 ┆ 24  ┆ [[28143]]                       │\n",
       "│ …         ┆ …   ┆ …   ┆ …                               │\n",
       "│ 73014     ┆ 56  ┆ 29  ┆ [[76319]]                       │\n",
       "│ 85995     ┆ 150 ┆ 88  ┆ [[350038]]                      │\n",
       "│ 353765    ┆ 200 ┆ 46  ┆ [[377121]]                      │\n",
       "│ 288885    ┆ 107 ┆ 7   ┆ [[273944]]                      │\n",
       "│ 359401    ┆ 146 ┆ 132 ┆ [[346288]]                      │\n",
       "└───────────┴─────┴─────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes_agg = pl.concat([\n",
    "    extract_articulations(fp, schema_prefix).with_columns(\n",
    "        pl.col(\"articulation\")\n",
    "        .map_elements(to_dnf, return_dtype=pl.List(pl.List(pl.Int64)))\n",
    "    ).lazy()\n",
    "    for fp\n",
    "    in PROJECTDIR.glob(\"data/*/*-prefixes.json\")\n",
    "]).collect().unique()\n",
    "\n",
    "prefixes_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "251e0c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with (PROJECTDIR/\"etl-pipeline/schema_prefix.pickle\").open(mode=\"wb\") as fp:\n",
    "    pickle.dump(obj=schema_prefix, file=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103b89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
